# -*- coding: utf-8 -*-
"""RAG_Q&A_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-Ar29eBt4y_4fbM2ZSaTMSzRsicIhVv
"""

import streamlit as st
import gdown
import tempfile
import os
import logging
import torch
from transformers import AutoTokenizer
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.fastembed import FastEmbedEmbedding
from llama_index.core import Settings, PromptTemplate

# Streamlit App Configuration
st.title("Q&A Assistant with Llama and HuggingFace")

# Install required dependencies
with st.spinner("Installing required packages..."):
    os.system('pip install -U transformers pypdf python-dotenv llama-index==0.10.12 gradio einops accelerate llama-index-llms-huggingface llama-index-embeddings-fastembed')

# Hugging Face login
huggingface_api_key = "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"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = huggingface_api_key

# Logging configuration
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Download and load documents using gdown
st.write("Downloading data...")
with tempfile.TemporaryDirectory() as temp_dir:
    url = 'https://drive.google.com/uc?id=1iZ1WH40Ck_d-gw0z4c7Pb4Qp50s9wKBC/view?usp=sharing'
    output = os.path.join(temp_dir, 'data.zip')
    gdown.download(url, output, quiet=False)

    # Unzip if necessary
    os.system(f'unzip {output} -d {temp_dir}')

    # Load the documents
    st.write("Loading documents...")
    documents = SimpleDirectoryReader(temp_dir).load_data()

# Set up embedding model
embed_model = FastEmbedEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.embed_model = embed_model
Settings.chunk_size = 512

# Set up system prompts
system_prompt = "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided."
query_wrapper_prompt = PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")
stopping_ids = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>"),
]

# Set up HuggingFaceLLM
llm = HuggingFaceLLM(
    context_window=8192,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.7, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Meta-Llama-3.1-8B-Instruct",
    model_name="meta-llama/Meta-Llama-3.1-8B-Instruct",
    device_map="auto",
    stopping_ids=stopping_ids,
    tokenizer_kwargs={"max_length": 4096},
    model_kwargs={"torch_dtype": torch.float16}
)

Settings.llm = llm
Settings.chunk_size = 512

# Create the index
st.write("Indexing documents...")
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# Function to clean response
def clean_response(response):
    clean_text = response.replace("<|ASSISTANT|>", "").replace("<|USER|>", "").replace("<|END_ASSISTANT|>", "").strip()
    clean_text = clean_text.replace("|#|", "").strip()
    if "|" in clean_text:
        clean_text = clean_text.split("|")[0].strip()

    return clean_text

# Function to predict the response
def predict(input_text):
    response = query_engine.query(input_text)
    cleaned_response = clean_response(str(response))
    return str(cleaned_response)

# Streamlit UI for user input
st.write("Ask a question based on the provided documents:")

user_input = st.text_input("Enter your question:")

if user_input:
    with st.spinner("Generating response..."):
        answer = predict(user_input)
        st.write("Answer:")
        st.write(answer)